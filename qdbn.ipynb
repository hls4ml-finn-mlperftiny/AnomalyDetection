{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 07:00:12.567341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:12.577561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:12.578256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from qkeras import QActivation, quantized_relu, QDense, quantized_bits, QDenseBatchnorm\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "import setGPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "if physical_devices: \n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "def get_model_ut(input_shape):\n",
    "    model = Sequential() \n",
    "    model.add(QDenseBatchnorm(16, input_shape=input_shape, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                               bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2dbn0\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu0\"))\n",
    "    \n",
    "    model.add(QDenseBatchnorm(16, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                               bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2dbn1\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu1\"))\n",
    "    \n",
    "    model.add(QDenseBatchnorm(24, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                               bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2dbn2\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu2\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(QDense(42, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform', name=\"dense0\"))\n",
    "\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu3\"))\n",
    "    \n",
    "    model.add(QDense(64, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform', name=\"dense1\"))\n",
    "\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu4\"))\n",
    "    \n",
    "    model.add(QDense(5, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform'))\n",
    "    \n",
    "    model.add(Activation(activation='softmax', name='softmax'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_ref(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(QDense(16, input_shape=input_shape, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                      bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2d0\"))\n",
    "    model.add(BatchNormalization(name=\"bn0\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu0\"))\n",
    "    \n",
    "    model.add(QDense(16, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                      bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2d1\"))\n",
    "    model.add(BatchNormalization(name=\"bn1\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu1\"))\n",
    "    \n",
    "    model.add(QDense(24, kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                      bias_quantizer=quantized_bits(16, 6, alpha=1), name=\"qc2d2\"))\n",
    "    model.add(BatchNormalization(name=\"bn2\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu2\"))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(QDense(42, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform', name=\"dense0\"))\n",
    "    # model.add(BatchNormalization(name=\"bn3\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu3\"))\n",
    "    \n",
    "    model.add(QDense(64, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform', name=\"dense1\"))\n",
    "    # model.add(BatchNormalization(name=\"bn4\"))\n",
    "    model.add(QActivation(quantized_relu(6), name=\"relu4\"))\n",
    "    \n",
    "    model.add(QDense(5, kernel_quantizer=quantized_bits(6, 0, alpha=1), bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                     kernel_initializer='lecun_uniform'))\n",
    "    model.add(Activation(activation='softmax', name='softmax'))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def dummy_model_ut(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        QDenseBatchnorm(16, input_shape=input_shape,\n",
    "                         kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=96),\n",
    "                         bias_initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=96),\n",
    "                         beta_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                         gamma_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                         moving_mean_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                         moving_variance_initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=4, seed=96),\n",
    "                         kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                         bias_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                         name=\"qdensebatchnorm\")\n",
    "        )\n",
    "    \n",
    "    model.build(input_shape=input_shape)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def dummy_model_ref(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        QDense(16, input_shape=input_shape,\n",
    "                 kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=96),\n",
    "                 bias_initializer=tf.keras.initializers.RandomUniform(minval=-1, maxval=1, seed=96),\n",
    "                 kernel_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                 bias_quantizer=quantized_bits(16, 6, alpha=1),\n",
    "                 name=\"qdense\")\n",
    "        )\n",
    "    model.add(\n",
    "        BatchNormalization(\n",
    "                 beta_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                 gamma_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                 moving_mean_initializer=tf.keras.initializers.RandomUniform(minval=-4, maxval=4, seed=96),\n",
    "                 moving_variance_initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=4, seed=96),\n",
    "                 name='bn')\n",
    "    )\n",
    "    \n",
    "    model.build(input_shape=input_shape)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "input_shape = (16,)\n",
    "\n",
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-09 07:00:33.532261: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-09 07:00:33.533661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:33.534450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:33.535255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:34.136004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:34.136659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:34.137249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-09 07:00:34.137821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14634 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute error:  1041.022216796875\n",
      "Mean relative error: 1314.5424842834473 %\n"
     ]
    }
   ],
   "source": [
    "dummy_ref = dummy_model_ref(input_shape)\n",
    "dummy_ut  = dummy_model_ut(input_shape)\n",
    "\n",
    "y_ref = dummy_ref.predict(X_test[:1000])\n",
    "y_ut  = dummy_ut.predict(X_test[:1000])\n",
    "\n",
    "max_abs_error = (np.abs(y_ref - y_ut)).max()\n",
    "mean_rel_error = (np.abs(y_ut[y_ut != 0] - y_ref[y_ut != 0]) / np.abs(y_ut[y_ut != 0])).mean() * 100\n",
    "\n",
    "print('Max absolute error:  {}'.format(max_abs_error))\n",
    "print('Mean relative error: {} %'.format(mean_rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tiny_qdense_batchnorm/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3891/3891 [==============================] - 30s 7ms/step - loss: 0.9855 - accuracy: 0.6524 - val_loss: 0.8486 - val_accuracy: 0.7051\n",
      "Epoch 2/30\n",
      "3891/3891 [==============================] - 28s 7ms/step - loss: 0.8775 - accuracy: 0.6889 - val_loss: 0.8333 - val_accuracy: 0.7066\n",
      "Epoch 3/30\n",
      "3891/3891 [==============================] - 28s 7ms/step - loss: 0.8588 - accuracy: 0.6957 - val_loss: 0.8270 - val_accuracy: 0.7078\n",
      "Epoch 4/30\n",
      "3891/3891 [==============================] - 28s 7ms/step - loss: 0.8503 - accuracy: 0.6983 - val_loss: 0.8259 - val_accuracy: 0.7076\n",
      "Epoch 5/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8433 - accuracy: 0.7001 - val_loss: 0.8206 - val_accuracy: 0.7093\n",
      "Epoch 6/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8390 - accuracy: 0.7010 - val_loss: 0.8183 - val_accuracy: 0.7091\n",
      "Epoch 7/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8337 - accuracy: 0.7022 - val_loss: 0.8203 - val_accuracy: 0.7093\n",
      "Epoch 8/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8305 - accuracy: 0.7034 - val_loss: 0.8161 - val_accuracy: 0.7080\n",
      "Epoch 9/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8258 - accuracy: 0.7050 - val_loss: 0.8058 - val_accuracy: 0.7135\n",
      "Epoch 10/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8236 - accuracy: 0.7054 - val_loss: 0.8037 - val_accuracy: 0.7132\n",
      "Epoch 11/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8204 - accuracy: 0.7063 - val_loss: 0.8031 - val_accuracy: 0.7133\n",
      "Epoch 12/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8181 - accuracy: 0.7072 - val_loss: 0.7991 - val_accuracy: 0.7150\n",
      "Epoch 13/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8145 - accuracy: 0.7081 - val_loss: 0.7955 - val_accuracy: 0.7161\n",
      "Epoch 14/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8107 - accuracy: 0.7096 - val_loss: 0.7920 - val_accuracy: 0.7172\n",
      "Epoch 15/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8087 - accuracy: 0.7097 - val_loss: 0.7912 - val_accuracy: 0.7174\n",
      "Epoch 16/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8056 - accuracy: 0.7109 - val_loss: 0.7911 - val_accuracy: 0.7171\n",
      "Epoch 17/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8024 - accuracy: 0.7118 - val_loss: 0.7883 - val_accuracy: 0.7177\n",
      "Epoch 18/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.8004 - accuracy: 0.7133 - val_loss: 0.7884 - val_accuracy: 0.7175\n",
      "Epoch 19/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7970 - accuracy: 0.7141 - val_loss: 0.7899 - val_accuracy: 0.7179\n",
      "Epoch 20/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7949 - accuracy: 0.7145 - val_loss: 0.7802 - val_accuracy: 0.7207\n",
      "Epoch 21/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7940 - accuracy: 0.7154 - val_loss: 0.7824 - val_accuracy: 0.7196\n",
      "Epoch 22/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7906 - accuracy: 0.7164 - val_loss: 0.7762 - val_accuracy: 0.7225\n",
      "Epoch 23/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7895 - accuracy: 0.7168 - val_loss: 0.7751 - val_accuracy: 0.7228\n",
      "Epoch 24/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7884 - accuracy: 0.7175 - val_loss: 0.7756 - val_accuracy: 0.7230\n",
      "Epoch 25/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7858 - accuracy: 0.7185 - val_loss: 0.7710 - val_accuracy: 0.7238\n",
      "Epoch 26/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7833 - accuracy: 0.7193 - val_loss: 0.7772 - val_accuracy: 0.7219\n",
      "Epoch 27/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7821 - accuracy: 0.7196 - val_loss: 0.7754 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7808 - accuracy: 0.7201 - val_loss: 0.7658 - val_accuracy: 0.7264\n",
      "Epoch 29/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7784 - accuracy: 0.7210 - val_loss: 0.8087 - val_accuracy: 0.7144\n",
      "Epoch 30/30\n",
      "3891/3891 [==============================] - 27s 7ms/step - loss: 0.7762 - accuracy: 0.7218 - val_loss: 0.7819 - val_accuracy: 0.7180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from qkeras.utils import load_qmodel\n",
    "import os\n",
    "\n",
    "def train_or_load(name, model):\n",
    "    if not os.path.isdir(name):\n",
    "        adam = Adam(lr=0.0001)\n",
    "        model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, batch_size=128,\n",
    "                  epochs=30, validation_split=0.25, shuffle=True,)\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(name + \"/KERAS_check_best_model.h5\",custom_objects={'quantized_relu':quantized_relu,'QDense': QDense, 'QActivation': QActivation, 'QDenseBatchnorm': QDenseBatchnorm})\n",
    "    return model\n",
    "\n",
    "ut_model_dir = \"model_ut_real\"\n",
    "ut_model  = train_or_load('keras_' + ut_model_dir, get_model_ut(input_shape))\n",
    "\n",
    "y_keras_ut = ut_model.predict(X_test)\n",
    "\n",
    "acc_keras_ut = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras_ut, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tiny_qdense_batchnorm/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3891/3891 [==============================] - 27s 6ms/step - loss: 0.9637 - accuracy: 0.6681 - val_loss: 0.8499 - val_accuracy: 0.7049\n",
      "Epoch 2/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8713 - accuracy: 0.6930 - val_loss: 0.8384 - val_accuracy: 0.7055\n",
      "Epoch 3/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8584 - accuracy: 0.6967 - val_loss: 0.8325 - val_accuracy: 0.7059\n",
      "Epoch 4/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8533 - accuracy: 0.6985 - val_loss: 0.8310 - val_accuracy: 0.7063\n",
      "Epoch 5/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8500 - accuracy: 0.6991 - val_loss: 0.8272 - val_accuracy: 0.7073\n",
      "Epoch 6/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8464 - accuracy: 0.7005 - val_loss: 0.8264 - val_accuracy: 0.7078\n",
      "Epoch 7/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8437 - accuracy: 0.7010 - val_loss: 0.8242 - val_accuracy: 0.7082\n",
      "Epoch 8/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8411 - accuracy: 0.7022 - val_loss: 0.8230 - val_accuracy: 0.7083\n",
      "Epoch 9/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8401 - accuracy: 0.7020 - val_loss: 0.8239 - val_accuracy: 0.7082\n",
      "Epoch 10/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8370 - accuracy: 0.7028 - val_loss: 0.8202 - val_accuracy: 0.7091\n",
      "Epoch 11/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8352 - accuracy: 0.7028 - val_loss: 0.8202 - val_accuracy: 0.7087\n",
      "Epoch 12/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8331 - accuracy: 0.7036 - val_loss: 0.8185 - val_accuracy: 0.7090\n",
      "Epoch 13/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8320 - accuracy: 0.7039 - val_loss: 0.8167 - val_accuracy: 0.7088\n",
      "Epoch 14/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8307 - accuracy: 0.7038 - val_loss: 0.8161 - val_accuracy: 0.7087\n",
      "Epoch 15/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8293 - accuracy: 0.7042 - val_loss: 0.8161 - val_accuracy: 0.7091\n",
      "Epoch 16/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8278 - accuracy: 0.7048 - val_loss: 0.8154 - val_accuracy: 0.7091\n",
      "Epoch 17/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8264 - accuracy: 0.7049 - val_loss: 0.8145 - val_accuracy: 0.7094\n",
      "Epoch 18/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8263 - accuracy: 0.7056 - val_loss: 0.8127 - val_accuracy: 0.7098\n",
      "Epoch 19/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8251 - accuracy: 0.7052 - val_loss: 0.8111 - val_accuracy: 0.7101\n",
      "Epoch 20/30\n",
      "3891/3891 [==============================] - 24s 6ms/step - loss: 0.8241 - accuracy: 0.7057 - val_loss: 0.8103 - val_accuracy: 0.7105\n",
      "Epoch 21/30\n",
      "3891/3891 [==============================] - 25s 6ms/step - loss: 0.8236 - accuracy: 0.7057 - val_loss: 0.8102 - val_accuracy: 0.7108\n",
      "Epoch 22/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8224 - accuracy: 0.7063 - val_loss: 0.8106 - val_accuracy: 0.7106\n",
      "Epoch 23/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8206 - accuracy: 0.7068 - val_loss: 0.8122 - val_accuracy: 0.7109\n",
      "Epoch 24/30\n",
      "3891/3891 [==============================] - 26s 7ms/step - loss: 0.8195 - accuracy: 0.7071 - val_loss: 0.8089 - val_accuracy: 0.7111\n",
      "Epoch 25/30\n",
      "3891/3891 [==============================] - 26s 7ms/step - loss: 0.8192 - accuracy: 0.7075 - val_loss: 0.8083 - val_accuracy: 0.7111\n",
      "Epoch 26/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8176 - accuracy: 0.7076 - val_loss: 0.8072 - val_accuracy: 0.7120\n",
      "Epoch 27/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8170 - accuracy: 0.7079 - val_loss: 0.8054 - val_accuracy: 0.7118\n",
      "Epoch 28/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8166 - accuracy: 0.7079 - val_loss: 0.8046 - val_accuracy: 0.7122\n",
      "Epoch 29/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8155 - accuracy: 0.7084 - val_loss: 0.8045 - val_accuracy: 0.7118\n",
      "Epoch 30/30\n",
      "3891/3891 [==============================] - 25s 7ms/step - loss: 0.8132 - accuracy: 0.7090 - val_loss: 0.8039 - val_accuracy: 0.7134\n"
     ]
    }
   ],
   "source": [
    "ref_model_dir = \"model_ref_real\"\n",
    "ref_model  = train_or_load('keras_' + ref_model_dir, get_model_ref(input_shape))\n",
    "\n",
    "y_keras_ref = ref_model.predict(X_test)\n",
    "\n",
    "acc_keras_ref = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras_ref, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras  model QDenseBatchnorm:    0.72\n",
      "Keras  model QDense + Batchnorm: 0.71\n"
     ]
    }
   ],
   "source": [
    "print('Keras  model QDenseBatchnorm:    {:,.2f}'.format(acc_keras_ut))\n",
    "print('Keras  model QDense + Batchnorm: {:,.2f}'.format(acc_keras_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny_qdense_batchnorm",
   "language": "python",
   "name": "tiny_qdense_batchnorm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
