train:
  dev_directory: ./dev_data
  eval_directory: ./eval_data
  feature:
      frames: 5
      hop_length: 512
      n_fft: 1024
      n_mels: 128
      power: 2.0
      downsample: True
  fit:
      batch_size: 512
      compile:
        loss: mean_squared_error
        optimizer: adam
      epochs: 100
      shuffle: true
      validation_split: 0.1
      verbose: 1
  max_fpr: 0.1
  model:
      input_dim: 64
      batch_norm: true
      latent_dim: 8
      hidden_dim: 64
      l1reg: 0
      name: qkeras_model
      encode_depth: 3
      encode_in: 64
      decode_depth: 3
      decode_out: 64
      quantization:
          bits: 10
          int_bits: 0
          last_bits: 10
          last_int_bits: 0
          relu_bits: 5
          relu_int_bits: 5
  model_directory: ./model/model_config/64input_depth3_x64_10b
  pruning:
      constant: false
      decay: false
      final_step: None
      initial_step: 0
      power: None
      sparsity: None
      initial_sparsity: None
      final_sparsity: None
  result_directory: ./result/model_config/64input_depth3_x64_10b
  result_file: result.csv
convert:
  x_npy_plot_roc: test_data/anomaly_detection/downsampled_128_5_to_32_2_skip_method.npy
  y_npy_plot_roc: test_data/anomaly_detection/downsampled_128_5_to_32_2_ground_truths_skip_method.npy
  x_npy_hls_test_bench: ./test_data/anomaly_detection/test_bench/downsampled_128_5_to_32_2_skip_method.npy
  y_npy_hls_test_bench: ./test_data/anomaly_detection/test_bench/downsampled_128_5_to_32_2_ground_truths_skip_method.npy
  model_file: model/model_config/64input_depth3_x64_10b/model_ToyCar.h5
  Build: True
  FIFO_opt: False
  OutputDir: hls/results/config/64input_depth3_x64_10b
  ClockPeriod: 10
  vivado_path: "/tools/Xilinx/Vivado/2019.1/bin:"
  Board: pynq-z2
  Trace: 0
  fpga_part: xc7z020clg400-1
  acc_name: anomaly_detection
  Backend: Vivado
  IOType: io_stream
  Interface: axi_stream
  Driver: python
  Strategy: Resource
  HLSConfig:
    LayerName:
      batch_normalization:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      batch_normalization_1:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_2:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_3:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_4:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_5:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_6:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      batch_normalization_7:
        Precision:
          bias: ap_fixed<16,6>
          scale: ap_fixed<16,6>
        ReuseFactor: 4096
        Trace: true
      input_1:
        Precision: ap_fixed<8,8>
        Trace: true
      q_activation:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_activation_1:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_2:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_3:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_4:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_5:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_6:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_activation_7:
        Precision:
          result: ap_fixed<6,6>
        ReuseFactor: 4096
        Trace: true
      q_dense:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_1:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_2:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_3:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_4:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_5:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_6:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
      q_dense_7:
        Precision:
          bias: ap_fixed<11,1>
          weight: ap_fixed<11,1>
        ReuseFactor: 4096
        Trace: true
        accum_t: ap_fixed<32,16>
    Model:
      Precision: ap_fixed<32,16>
      ReuseFactor: 4096
      Strategy: Resource
      FIFO_opt: true